# -*- coding: utf-8 -*-
import jieba
import spacy

print('load model\n')
nlp = spacy.load("zh_core_web_lg")

nlp('å£å‘³ä¸éŒ¯ï¼Œä¾¿ç•¶è£çš„æ»¿æ»¿å¯å¯å¯å¯ä»¥åƒé£½é£½ã€‚ç›®å‰ä¸é–‹æ”¾å…§ç”¨ï¼Œä¸éŽåº—è£¡é¢å¥½åƒæ²’å†·æ°£ï¼ŒçœŸè¦è£¡é¢åƒæœƒæ¯”è¼ƒç†±ã€‚')

with open('./stops.txt', 'r', encoding='utf8') as f:
    global stopwords
    stopwords = f.read().split('\n')
  
stopwords.append('\n')
stopwords.append('\n\n')
stopwords.append('\n\n\n')
stopwords.append('â‹¯')
stopwords.append('ðŸ˜†')

print('create stop words\n\n')
nlp.Defaults.stop_words |= set(stopwords)

print('stop words created\n')

# filt stopwordså’Œ ä¸ç›¸é—œçš„è©ž
def filtStopWords(documents):
    words = set()
    i = 0
    #print('hi')
   
    doc = jieba.cut(documents.replace(' ', ''))
    for token in doc:  
        if str(token) not in nlp.Defaults.stop_words:
            words.add(str(token))
       
    return words
ex = ['æ«ƒæª¯å®¢å®¢æ°£æ°£ï¼Œä¸”ç®—ç›¸å°é‡è¦–é˜²ç–«çš„åº—å®¶ï¼Œç–«æƒ…æœŸé–“åƒ…æä¾›å¤–å¸¶ã€‚\né‹ç”¨åœ¨åœ°ç‰¹è‰²é®ªé­šé†¬ç‚’å‡ºä¾†çš„é£¯å…¶å®ƒåœ°æ–¹æ‡‰è©²å¾ˆé›£åƒåˆ°ï¼Œè »å¤§ä¸€ä»½ä¹Ÿè »å¥½åƒçš„ï¼Œç²’ç²’åˆ†æ˜Žï¼Œé£¯ä¸æœƒéŽä¹¾ã€èœé‚„æ˜¯è„†çš„ï¼Œé®ªé­šé†¬è·Ÿé£¯ç¢ºç¢ºå¯¦å¯¦çµåˆåœ¨ä¸€èµ·ï¼Œä¸éŽå®ƒå€‹æ€§å¤ªå¼·ï¼Œæœ‰ç¨å¾®æ¶èµ°å…¶ä»–å…ƒç´ çš„å­˜åœ¨æ„Ÿã€‚\n\nè¨»ï¼šä»¥ä¸Šç‚ºæœ¬äººæ„Ÿæƒ³ï¼Œæ¯å€‹äººçš„è§€æ„Ÿä¸åŒï¼Œå¿ƒå¾—åƒ…ä¾›åƒè€ƒã€‚', 'ç‚’æ³¡éºµåŠ è¾£å¥½åƒï¼Œé‡é»žæ˜¯æœ€ä¸‹é¢é‚„æœƒè—ä¸€é¡†è·åŒ…è›‹ï¼Œæ¯æ¬¡åƒæ¯æ¬¡å¿˜è¨˜ï¼Œéºµä¸€æŒ–æ‰ç™¼ç¾ï¼Œåœ¨ç¶ å³¶ä¸€å€‹åŠæœˆæœ€å¸¸åƒçš„å°åƒ\nè‚‰çµ²ç‚’é£¯åæ²¹ï¼Œé®ªé­šé†¬ç‚’é£¯å¤ªé¹¹\né‹ç‡’çƒé¾æœ‰ä¸‰é¡†è›¤èœŠï¼Œcpå€¼é«˜', 'é®ªé­šé†¬ç‚’é£¯è »ç‰¹åˆ¥çš„ï¼Œæœ‰é»žè¾£ï¼Œ\næµ·è‰ç‚’è›‹åƒä¸å¤ªå‡ºç‰¹æ®Šä¹‹è™•ã€‚\nä»½é‡æ»¿å¤šçš„ä½†åƒ¹æ ¼ä¹Ÿä¸ä¾¿å®œ', 'å› ç–«æƒ…é—œä¿‚åªèƒ½å¤–å¸¶ï¼Œè¦å‰å¾€çš„æœ‹å‹ä¹Ÿå¯ä»¥è‡ªå‚™å®¹å™¨æ¸›å°‘ä¸€æ¬¡æ€§é¤ç›’ä½¿ç”¨ã€‚é®ªé­šé†¬ç‚’é£¯ä»½é‡è¶…å¤šï¼Œé£Ÿé‡å°çš„äººå¯ä»¥å…©äººå…±åˆ†ä¸€ä»½æ²’å•é¡Œï¼Œé®ªé­šç‚’è›‹ä¹Ÿæ˜¯ä»½é‡æ»¿å¤šï¼Œç‚’æ³¡éºµå’Œæ°´é¤ƒé›–ç„¶æ²’å‰é¢å…©é“é©šè‰·ä½†ä¹Ÿé‚„ä¸éŒ¯ï¼Œæ•´é«”ä¾†è¬›æ˜¯å€‹å¹³åƒ¹åˆå¥½åƒçš„å°åƒåº—ã€‚', 'å£å‘³ä¸éŒ¯ï¼Œä¾¿ç•¶è£çš„æ»¿æ»¿æ»¿å¯ä»¥åƒé£½é£½ã€‚ç›®å‰ä¸é–‹æ”¾å…§ç”¨ï¼Œä¸éŽåº—è£¡é¢å¥½åƒæ²’å†·æ°£ï¼ŒçœŸè¦è£¡é¢åƒæœƒæ¯”è¼ƒç†±ã€‚']

print('filter test:\n\n')

# åšembeddingä¸¦ç”¢ç”Ÿvectors
def word2vec(words):
    vectors = []
    for word in words:
        vectors.append(nlp(word).vector.tolist())
    return vectors

def word2vec2(words):
    return [nlp(word).vector.tolist() for word in words]


import pymongo

remoteUrl = "mongodb+srv://mark:WNQmnmMW1Eob4gFi@cluster0.gvyaavk.mongodb.net/?retryWrites=true&w=majority"
localUrl = "mongodb://localhost:27017"
myclient = pymongo.MongoClient(remoteUrl)
mydb = myclient["gp"]
mycol = mydb["map"]

print('connect mongo\n')

keywords = ["é¤å»³"]
restaurantType = ["æ—¥å¼","ç¾Žå¼","ä¸­å¼","ä¸­å¼é¤å»³"]
# food = ["æ¼¢å ¡","è”¥æ²¹é¤…","çç å¥¶èŒ¶","é£²æ–™","é£Ÿç‰©"]
foodArr = ["ç¾©å¤§åˆ©éºµ","é›žæŽ’","è±¬æŽ’","çƒ¤è‚‰","è•Žéº¥éºµ","ç”Ÿé­šç‰‡","ä¸¼é£¯","å£½å¸","éºµåŒ…","è›‹ç³•","è›‹åŒ…é£¯","ç‚’éºµ","ç‚’é£¯","é¤ƒå­","é¤…ä¹¾","éºµç·š","éºµ","æ¼¢å ¡","è–¯æ¢","ç‚¸é›ž","ç‚¸é­š","ç‚¸è¦","ç‰›æŽ’","ç‡’çƒ¤","ç«é‹","å£½å–œç‡’","ç‡’è‡˜","ç‡’è‚‰","æ¹¯åœ“","é‹è²¼","ç‡’é¤…","é£¯ç³°","ç‚’ç±³ç²‰","ç‚’ç±³ç³•","ç³¯ç±³é£¯","ç‡’è³£","ç‡’é´¨","ç‡’éµ","è±¬è…³","è±¬è‚‰","è±¬è…³","é¹¹é…¥é›ž","é‹ç‡’æ„éºµ","è”¥æ²¹é¤…","ç”œé»ž","èµ·å¸","å·§å…‹åŠ›","ç„—çƒ¤","æ²™æ‹‰","é…’"]

i = 1
count = 0
wordsdel = []
wordskept = []
nlpFood = nlp("é£Ÿç‰©")
nlpRestaraunt = nlp("é¤å»³")
nlpFoodArr = [ nlp(x) for x in foodArr]


for doc in mycol.find():
    #print(len(doc['reviews']))
    words = filtStopWords(doc['reviews'])
    # print("words: ",words)
    i += 1

    # test each word in words' similarity 

    phraseskept = []

    for word in words:

        similar = False
        
        nlpWord = nlp(word)
        if nlpWord.similarity(nlpFood) >= 0.149 and nlpWord.similarity(nlpRestaraunt) > 0.1: 
        # if nlpWord.similarity(nlpFood) >= 0.4 or nlpWord.similarity(nlp(keywords[0])) > 0.4: 
            # similar = True

            totalSimilarity = 0
            for food in nlpFoodArr:
                totalSimilarity += nlpWord.similarity(food)
            if(totalSimilarity/len(foodArr) >= 0.3):
                # print(word, "average similarity", totalSimilarity/len(foodArr))
                similar = True
        
        if similar:
            phraseskept.append({'phrase':word, 'similarity':totalSimilarity/len(foodArr)})
            count += 1
        else:
            wordsdel.append(word)

    # keep the first five words in sorted phrasekept to wordskept
    phraseskept.sort(key=lambda doc: doc['similarity'], reverse=True)
    # for phrase in phraseskept[:5]:
        # wordskept.append(phrase['phrase'])

    print(doc['name'])
    for word in phraseskept:
        print(word['phrase'],end=' ') 
    print('\n')
    

    vectors = word2vec(words)
    query = {"place_id": doc['place_id']}
    newvalues = {"$set": {"reviews_spacy": vectors}}
    mycol.update_one(query, newvalues)

    if i % 100 == 0:
        print(i)
        print(count)
        print('wordsdel: ',wordsdel)
        print('wordskept: ',wordskept)
        print('-------------------------------------')

myclient.close()
print(wordskept)
print(wordsdel)
print(count/len(wordsdel))




    
