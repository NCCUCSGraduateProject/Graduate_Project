# -*- coding: utf-8 -*-
import jieba
import spacy

print('load model\n')
nlp = spacy.load("zh_core_web_lg")

with open('./stops.txt', 'r', encoding='utf8') as f:
    global stopwords
    stopwords = f.read().split('\n')
  
stopwords.append('\n')
stopwords.append('\n\n')
stopwords.append('\n\n\n')
stopwords.append('â‹¯')
stopwords.append('ðŸ˜†')

print('create stop words\n\n')
nlp.Defaults.stop_words |= set(stopwords)

print('stop words created\n')

# filt stopwordså’Œ ä¸ç›¸é—œçš„è©ž
def filtStopWords(documents):
    words = set()
    i = 0
    #print('hi')
   
    doc = jieba.cut(documents.replace(' ', ''))
    for token in doc:  
        if str(token) not in nlp.Defaults.stop_words:
            words.add(str(token))
       
    return words

print('filter test:\n\n')

# åšembeddingä¸¦ç”¢ç”Ÿvectors
def word2vec(words):
    vectors = []
    for word in words:
        vectors.append(nlp(word).vector.tolist())
    return vectors

def word2vec2(words):
    return [nlp(word).vector.tolist() for word in words]


import pymongo

remoteUrl = "mongodb+srv://mark:WNQmnmMW1Eob4gFi@cluster0.gvyaavk.mongodb.net/?retryWrites=true&w=majority"
localUrl = "mongodb://localhost:27017"
myclient = pymongo.MongoClient(localUrl)
mydb = myclient["placeAPI"]
mycol = mydb["testTemp"]

print('connect mongo\n')

foodArr = ["ç¾©å¤§åˆ©éºµ","é›žæŽ’","è±¬æŽ’","çƒ¤è‚‰","è•Žéº¥éºµ","ç”Ÿé­šç‰‡","ä¸¼é£¯","å£½å¸","éºµåŒ…","è›‹ç³•","è›‹åŒ…é£¯","ç‚’éºµ","ç‚’é£¯","é¤ƒå­","é¤…ä¹¾","éºµç·š","éºµ","æ¼¢å ¡","è–¯æ¢","ç‚¸é›ž","ç‚¸é­š","ç‚¸è¦","ç‰›æŽ’","ç‡’çƒ¤","ç«é‹","å£½å–œç‡’","ç‡’è‡˜","ç‡’è‚‰","æ¹¯åœ“","é‹è²¼","ç‡’é¤…","é£¯ç³°","ç‚’ç±³ç²‰","ç‚’ç±³ç³•","ç³¯ç±³é£¯","ç‡’è³£","ç‡’é´¨","ç‡’éµ","è±¬è…³","è±¬è‚‰","è±¬è…³","é¹¹é…¥é›ž","é‹ç‡’æ„éºµ","è”¥æ²¹é¤…","ç”œé»ž","èµ·å¸","å·§å…‹åŠ›","ç„—çƒ¤","æ²™æ‹‰","é…’"]

i = 1
count = 0   
wordsdel = []
wordskept = []
nlpScenery = nlp("æ™¯é»ž")
nlpLandscape = nlp("é¢¨æ™¯")
nlpSea = nlp("æµ·")

nlpFoodArr = [ nlp(x) for x in foodArr]


for doc in mycol.find():
    
    if "tourist_attraction" in doc["types"] or "amusement_park" in doc["types"]:
        
        words = filtStopWords(doc['reviews'])
        print(words)
        i += 1

        # test each word in words' similarity 

        phraseskept = []

        for word in words:
            nlpWord = nlp(word)
            print(word, round(nlpWord.similarity(nlpScenery),3), round(nlpWord.similarity(nlpLandscape),3), round(nlpWord.similarity(nlpSea),3))

        words = []
        for word in words:

            similar = False
            
            nlpWord = nlp(word)
            if nlpWord.similarity(nlpScenery) >= 0.149 and nlpWord.similarity(nlpLandscape) > 0.1: 
            # if nlpWord.similarity(nlpFood) >= 0.4 or nlpWord.similarity(nlp(keywords[0])) > 0.4: 
                # similar = True

                totalSimilarity = 0
                for food in nlpFoodArr:
                    totalSimilarity += nlpWord.similarity(food)
                if(totalSimilarity/len(foodArr) >= 0.3):
                    # print(word, "average similarity", totalSimilarity/len(foodArr))
                    similar = True
            
            if similar:
                phraseskept.append(word)
                count += 1
            else:
                wordsdel.append(word)

        # keep the first five words in sorted phrasekept to wordskept
        # phraseskept.sort(key=lambda doc: doc['similarity'], reverse=True)
        # for phrase in phraseskept[:5]:
            # wordskept.append(phrase['phrase'])

    

myclient.close()
print(wordskept)
print(wordsdel)




    
